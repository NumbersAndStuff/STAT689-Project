{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "import ast # this is just use to evaluate the lemmas\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loads the required default english model used to tokenize words\n",
    "# this must be downloaded previously \n",
    "# python -m spacy download en\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner'])\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell takes awhile to run.  As such, the results were saved in a new \n",
    "# .csv all cells below are self-contained without the first three cells\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "dep = []\n",
    "\n",
    "# where pos is parts of speech and dep is dependency \n",
    "\n",
    "for doc in nlp.pipe(df['text'].astype('unicode').values, batch_size=205000,\n",
    "                        n_threads=4):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "        dep.append([n.dep_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "        dep.append(None)\n",
    "\n",
    "df['text_tokens'] = tokens\n",
    "df['text_lemma'] = lemma\n",
    "df['text_pos'] = pos\n",
    "df['text_dep'] = dep\n",
    "\n",
    "df.to_csv('tweets_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets_parsed.csv\", low_memory=False, index_col=0, dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this statement just converts the str(list) to a list of strings\n",
    "\n",
    "tweets['tokens'] = tweets['text_tokens'].apply(ast.literal_eval)\n",
    "tweets['lemma'] = tweets['text_lemma'].apply(ast.literal_eval)\n",
    "tweets['pos'] = tweets['text_pos'].apply(ast.literal_eval)\n",
    "tweets['dep'] = tweets['text_dep'].apply(ast.literal_eval)\n",
    "\n",
    "print(tweets.text[25])\n",
    "print(tweets.tokens[25])\n",
    "print(tweets.lemma[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"train.csv\", index_col=False, encoding='latin-1', header=0)\n",
    "\n",
    "\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "dep = []\n",
    "\n",
    "for doc in nlp.pipe(df['text'].astype('unicode').values, batch_size=500,\n",
    "                        n_threads=4):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "        dep.append([n.dep_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "        dep.append(None)\n",
    "\n",
    "df['text_tokens'] = tokens\n",
    "df['text_lemma'] = lemma\n",
    "df['text_pos'] = pos\n",
    "df['text_dep'] = dep\n",
    "\n",
    "df.to_csv('training_parsed2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = pd.read_csv(\"training_parsed.csv\", low_memory=False, index_col=0, dtype='object')\n",
    "\n",
    "ts['tokens'] = ts['text_tokens'].apply(ast.literal_eval)\n",
    "ts['lemma'] = ts['text_lemma'].apply(ast.literal_eval)\n",
    "ts['pos'] = ts['text_pos'].apply(ast.literal_eval)\n",
    "ts['dep'] = ts['text_dep'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ts_new = ts[:80000]\n",
    "ts_rest = ts[~ts.index.isin(ts_new.index)]\n",
    "ts_rest = ts[~ts.ItemID.isin(ts_new.ItemID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit_transform fits the transform and transforms, while .transform \n",
    "# applies that fit to the rest of the datasets\n",
    "# this is the weighting and normalizing \n",
    "\n",
    "features = vectorizer.fit_transform(ts_new.lemma.astype('unicode'))\n",
    "#print(vectorizer.vocabulary_)\n",
    "smatrix = vectorizer.transform(ts_rest.lemma.astype('unicode'))\n",
    "#print(smatrix)\n",
    "\n",
    "\n",
    "y = ts_new['Sentiment']\n",
    "X = features\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "t0 = time.time()\n",
    "\n",
    "#building the tree\n",
    "dtf = dt.fit(X,y)\n",
    "\n",
    "t1 = time.time()\n",
    "#testing the tree\n",
    "predict_dt = dt.predict(smatrix)\n",
    "t2 = time.time()\n",
    "print('time to train '+str(t1-t0))\n",
    "print('time to predict '+str(t2-t1))\n",
    "\n",
    "print(classification_report(ts_rest.Sentiment, predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applying the same transform to the troll tweets \n",
    "# and running the corpus through the tree\n",
    "\n",
    "tweet_matrix = vectorizer.transform(tweets.text_lemma)\n",
    "\n",
    "predict_tweets = dt.predict(tweet_matrix)\n",
    "tweet_prob = dt.predict_proba(tweet_matrix)\n",
    "#tweet_score = dt.score(tweet_matrix,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(dtf.feature_importances_, 'o')\n",
    "#plt.ylim(0.005,0.051)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = dtf.feature_importances_.tolist()\n",
    "out = pd.Series(b)\n",
    "out = pd.DataFrame({'x':out.index, 'y':out.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_cut = out[out.y>=.01]\n",
    "out_cut.reset_index()\n",
    "print(out_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this flips the dictionary so that we can call the index\n",
    "maps = {value: key for key, value in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pulls out the pertinent terms\n",
    "\n",
    "def get(key, d=maps, default=None):\n",
    "    \n",
    "    if key in d:\n",
    "        return d[key]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "\n",
    "abc = out_cut['x'].apply(get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc = pd.DataFrame({'key':abc.index,'vocab':abc.values},index=out_cut.index)\n",
    "abc['weight'] = out_cut.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same algorithm as for the decision tree, but the forest instead\n",
    "\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(X,y)\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "rf_predicta = rf.predict(smatrix)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)\n",
    "accuracy = accuracy_score(ts_rest.Sentiment,rf_predicta)\n",
    "\n",
    "print(classification_report(ts_rest.Sentiment, predict_dt))\n",
    "print(classification_report(ts_rest.Sentiment, rf_predicta))\n",
    "print(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\n",
    "print(f'Mean accuracy score: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "rf_predict = rf.predict(tweet_matrix)\n",
    "t1 = time.time()\n",
    "\n",
    "print(t1-t0)\n",
    "#accuracy = accuracy_score\n",
    "#print(classification_report(ts_rest.Sentiment, predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# plotting the confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(ts_rest.sentiment, rf_predicta))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
